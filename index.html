
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>HIVE: Harnessing Human Feedback for Instructional Visual Editing</title>

<!-- Meta tags for search engines to crawl -->
<meta name="robots" content="index,follow">
<meta name="description" content="HIVE"
>
<meta name="keywords" content="image editing; instructional image editing; human feedback;">
<link rel="author" href="https://shugerdou.github.io/">

<!-- Fonts and stuff -->
<link href=".projects/css" rel="stylesheet" type="text/css">
<link rel="stylesheet" type="text/css" href="./projects/project.css" media="screen">
<link rel="stylesheet" type="text/css" media="screen" href="./projects/iconize.css">

<style>
  .layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
    box-shadow:
            0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
            5px 5px 0 0px #fff, /* The second layer */
            5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
            10px 10px 0 0px #fff, /* The third layer */
            10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
            15px 15px 0 0px #fff, /* The fourth layer */
            15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
            20px 20px 0 0px #fff, /* The fifth layer */
            20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
            25px 25px 0 0px #fff, /* The fifth layer */
            25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
    margin-left: 10px;
    margin-right: 45px;
  }
</style>

<script async="" src="./prettify.js"></script>

</head>

<body>
  <div id="content">
    <div id="content-inner">
      
    <div class="section head">
    <h1><a href="https://arxiv.org/pdf/2303.09618.pdf" target="_blank" rel="nofollow">HIVE: Harnessing Human Feedback for Instructional Visual Editing</a></h1>
    <br>

    <div class="authors">
      <a target="_blank" rel="nofollow">Shu Zhang</a><sup>*1</sup>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
      <a target="_blank" rel="nofollow">Xinyi Yang</a><sup>*1</sup>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
      <a target="_blank" rel="nofollow">Yihao Feng</a><sup>*1</sup>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
      <a target="_blank" rel="nofollow">Can Qin</a><sup>3</sup>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
      <a target="_blank" rel="nofollow"> Chia-Chih Chen</a><sup>1</sup>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
      <a target="_blank" rel="nofollow"> Ning Yu</a><sup>1</sup>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
      <a target="_blank" rel="nofollow"> Zeyuan Chen</a><sup>1</sup>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
      <a target="_blank" rel="nofollow"> Huan Wang</a><sup>1</sup>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
      <a target="_blank" rel="nofollow"> Silvio Savarese</a><sup>1,2</sup>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
      <a target="_blank" rel="nofollow">  Stefano Ermon</a><sup>2</sup>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
      <a target="_blank" rel="nofollow"> Caiming Xiong</a><sup>1</sup>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
      <a target="_blank" rel="nofollow"> Ran Xu</a><sup>1</sup>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
      

    </div>

    <div class="affiliations">
      1. Salesforce Research&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
      2. Stanford University&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
      3. Northeastern University
    </div>
      
    <p class="is-size-7 is-text-centered">
              *Denotes equal contribution
	      <br><br>
    </p>
</div>
      
    <center><img src="./imgs/method.png" border="0" width=800px></center>

<div class="section abstract">
    <h2>Abstract</h2>
    <br>
    Incorporating human feedback has been shown to be crucial to align text generated by large language models to human preferences. We hypothesize that state-of-the-art instructional image editing models, where outputs are generated based on an input image and an editing instruction, could similarly benefit from human feedback, as their outputs may not adhere to the correct instructions and preferences of users. In this paper, we present a novel framework to harness human feedback for instructional visual editing (HIVE). Specifically, we collect human feedback on the edited images and learn a reward function to capture the underlying user preferences. We then introduce scalable diffusion model fine-tuning methods that can incorporate human preferences based on the estimated reward. Besides, to mitigate the bias brought by the limitation of data, we contribute a new 1M training dataset, a 3.6K reward dataset for rewards learning, and a 1K evaluation dataset to boost the performance of instructional image editing. We conduct extensive empirical experiments quantitatively and qualitatively, showing that HIVE is favored over previous state-of-the-art instructional image editing approaches by a large margin.
</div>

<div class="section results">
  <h2>Results</h2>
  <center><img src="./imgs/result1.png" border="0" width="94%"></center>
  <h3>From left to right: Input image, HIVE without human feedback, HIVE with human feedback.</h3></center>
  <h3>Result 2</h3>
  <center><img src="./TextureMixer/texture_dissolve_animal.gif" border="0" width="45%">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<img src="./TextureMixer/texture_dissolve_plant.gif" border="0" width="45%"></center>
  <h3>Result 3</h3>
  <center><img src="./TextureMixer/texture_brush_animal.gif" border="0" width="90%"></center>
  <br>
  <center><img src="./TextureMixer/texture_brush_earth.gif" border="0" width="90%"></center>
  <br>
  <center><img src="./TextureMixer/texture_brush_plant.gif" border="0" width="90%"></center>
  <h3>Result 4</h3>
  <center><img src="./TextureMixer/beardog.png" border="0" width="90%"></center>
  <br>
  <center><img src="./TextureMixer/leoraffe.png" border="0" width="90%"></center>
  <br>
  <center><img src="./TextureMixer/ziger.png" border="0" width="90%"></center>
</div>

<div class="section paper">
  <h2>Paper</h2>
  <center>
    <table style="border-collapse: collapse; border: none;"><tbody>
      <tr style="border: none;">
        <td style="border: none;">
          <center>
            <a href="https://arxiv.org/pdf/2303.09618.pdf" target="_blank" rel="nofollow"><img class="layered-paper-big" style="height:175px" src="./imgs/paper.png"></a>
            <br>
            <br>
            <br>
            <a href="https://arxiv.org/pdf/2303.09618.pdf" target="_blank" rel="nofollow">Paper</a>
          </center>
        </td>
      </tr>
    </tbody></table>
</center>
</div>
<br>

<div class="section code">
  <h2>Code</h2>
  <center>
    <a href="https://github.com/salesforce/HIVE" target="_blank" rel="nofollow"><img src="./imgs/Github.png" width="270 px"></a>
  </center>
</div>
<br>


<div class="section citation">
  <h2>Citation</h2>
  <div class="section bibtex">
    <pre>@inproceedings{zhang2022hive,
  author = {Zhang, Shu and Yang, Xinyi and Feng, Yihao and Qin, Can and Chen, Chia-Chih and Yu, Ning and Chen, Zeyuan and Wang, Huan and Savarese, Silvio and Ermon, Stefano and Xiong, Caiming and Xu, Ran},
  title = {HIVE: Harnessing Human Feedback for Instructional Visual Editing},
  journal={arXiv preprint arXiv:2303.09618},
  year = {2023}
}</pre>
  </div>
</div>

<div class="section acknowledgement">
  <h2>Acknowledgement</h2>
  <br>
  We thank the paper  <a href="https://github.com/timothybrooks/instruct-pix2pix/" target="_blank" rel="nofollow">InstructPix2Pix</a>, and our code is built upon it.
</div>


</body></html>
